{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "import h5py\n",
    "import scipy\n",
    "\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load MNIST Data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    f.seek(0)\n",
    "    \n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "    return (training_data, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the data looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([5, 0, 4, ..., 8, 4, 8], dtype=int64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that each dataset is represented by a tuple comprising of 2 arrays :\n",
    "\n",
    "  - first element represents pixelated images where each element has a length of 784 (28 X 28)\n",
    "  - second element represents true labels of individual images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "# shape of data\n",
    "print(training_data[0].shape)\n",
    "print(training_data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature dataset : [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Target dataset : [5 0 4 ... 8 4 8]\n",
      "\n",
      "Number of examples in the training dataset : 50000\n",
      "Number of points in a single input : 784\n",
      "\n",
      "Number of elements in the target dataset : 50000\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature dataset : {}\".format(training_data[0]))\n",
    "print(\"Target dataset : {}\\n\".format(training_data[1]))\n",
    "\n",
    "print(\"Number of examples in the training dataset : {}\".format(len(training_data[0])))\n",
    "print(\"Number of points in a single input : {}\\n\".format(len(training_data[0][1])))\n",
    "\n",
    "print(\"Number of elements in the target dataset : {}\".format(len(training_data[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> -  *Number of images present in the training dataset : 50,000*\n",
    "-  *Each image is represented by 784 data points representing a 28 X 28 image*\n",
    "-  *Each element in target dataset represents expected image label*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preparation\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert output image label into one-hot encoded column vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(j):\n",
    "    # get the number of elements in the input array\n",
    "    n = j.shape[0]\n",
    "\n",
    "    # creates a 10 X n array \n",
    "    # each row represents a digit i.e. 10 rows (0 - 9)\n",
    "    # each column represents an input\n",
    "    new_array = np.zeros((10, n))\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    for res in j:\n",
    "        new_array[res][index] = 1.0\n",
    "        index = index + 1\n",
    "    \n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert datasets into desired shapes and the ground truth labels to one_hot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_wrapper(tr_d, va_d, te_d):\n",
    "\n",
    "    # transpose input matrix so that each input is represented by a column vector\n",
    "    training_inputs = np.array(tr_d[0][:]).T\n",
    "    validation_inputs = np.array(va_d[0][:]).T\n",
    "    test_inputs = np.array(te_d[0][:]).T\n",
    "\n",
    "    # generate one-hot encoded matrix for output labels\n",
    "    training_results = np.array(tr_d[1][:])\n",
    "    train_set_y = one_hot(training_results)\n",
    "\n",
    "    validation_results = np.array(va_d[1][:])\n",
    "    validation_set_y = one_hot(validation_results)\n",
    "\n",
    "    test_results = np.array(te_d[1][:])\n",
    "    test_set_y = one_hot(test_results)\n",
    "\n",
    "    return (training_inputs, train_set_y, test_inputs, test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x, train_set_y, test_set_x, test_set_y = data_wrapper(\n",
    "    training_data, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x shape: (784, 50000)\n",
      "train_set_y shape: (10, 50000)\n",
      "test_set_x shape: (784, 10000)\n",
      "test_set_y shape: (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "print (\"train_set_x shape: \" + str(train_set_x.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data_wrapper has converted the training and validation data into numpy array of desired shapes. Let's convert the actual labels into a dataframe to see if the one hot conversions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target dataset is:[5 0 4 ... 8 4 8]\n",
      "The one hot encoding dataset is:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>49990</th>\n",
       "      <th>49991</th>\n",
       "      <th>49992</th>\n",
       "      <th>49993</th>\n",
       "      <th>49994</th>\n",
       "      <th>49995</th>\n",
       "      <th>49996</th>\n",
       "      <th>49997</th>\n",
       "      <th>49998</th>\n",
       "      <th>49999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 50000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3      4      5      6      7      8      9      \\\n",
       "0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1    0.0    0.0    0.0    1.0    0.0    0.0    1.0    0.0    1.0    0.0   \n",
       "2    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0   \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0   \n",
       "4    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0   \n",
       "5    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "6    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "7    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "8    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "9    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   ...    49990  49991  49992  49993  49994  49995  49996  49997  49998  49999  \n",
       "0  ...      0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0  \n",
       "1  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2  ...      0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4  ...      0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    1.0    0.0  \n",
       "5  ...      0.0    1.0    1.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0  \n",
       "6  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "7  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "8  ...      1.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    1.0  \n",
       "9  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[10 rows x 50000 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The target dataset is:\" + str(training_data[1]))\n",
    "print(\"The one hot encoding dataset is:\")\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualise the dataset. Feel free to change the index to see if the training data has been correctly tagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21e01e3e048>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index  = 49991\n",
    "k = train_set_x[:,index]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label= training_data[1][index]))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Activation Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid\n",
    "It takes a cumulative input into the layer, the matrix __Z__. Upon application of `sigmoid` function, the output matrix **H** is calculated. Also, Z is stored as the variable `sigmoid_memory` since it will be later used in backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "\n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples\n",
    "    # sigmoid_memory is stored as it is used later on in backpropagation\n",
    "\n",
    "    H = 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    sigmoid_memory = Z\n",
    "\n",
    "    return H, sigmoid_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5        0.88079708 0.98201379 0.99752738]\n",
      "[0.5        0.73105858]\n"
     ]
    }
   ],
   "source": [
    "# 4 neurons in a layer, 2 input data points\n",
    "Z = np.arange(8).reshape(4, 2)\n",
    "H, sigmoid_memory = sigmoid(Z)\n",
    "\n",
    "# output of 1st input data points\n",
    "print(H[:, 0])\n",
    "\n",
    "# output of top neuron\n",
    "print(H[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relu\n",
    "It takes the cumulative input to the layer, matrix **Z**. Upon application of the `relu` function, matrix __H__ which is the output matrix is calculated. Also, Z is stored as `relu_memory` which will be later used in backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples\n",
    "    # relu_memory is stored as it is used later on in backpropagation\n",
    "\n",
    "    H = np.maximum(0, Z)\n",
    "\n",
    "    assert (H.shape == Z.shape)\n",
    "\n",
    "    relu_memory = Z\n",
    "    return H, relu_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulative input (Z) : [[ 1  3]\n",
      " [-1 -4]\n",
      " [-5  7]\n",
      " [ 9 18]]\n",
      "\n",
      "output : [[ 1  3]\n",
      " [ 0  0]\n",
      " [ 0  7]\n",
      " [ 9 18]]\n"
     ]
    }
   ],
   "source": [
    "# 4 neurons in a layer, 2 input data points\n",
    "Z = np.array([1, 3, -1, -4, -5, 7, 9, 18]).reshape(4, 2)\n",
    "\n",
    "H, relu_memory = relu(Z)\n",
    "\n",
    "# cumulative input to the layer\n",
    "print(\"cumulative input (Z) : {}\\n\".format(relu_memory))\n",
    "\n",
    "# output of relu activation function\n",
    "print(\"output : {}\".format(H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax\n",
    "It takes the cumulative input to the layer, matrix **Z**. Upon application of the `softmax` function, the output matrix __H__ is calculated. Also, Z is stored as `softmax_memory` which will be later used in backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples\n",
    "    # softmax_memory is stored as it is used later on in backpropagation\n",
    "\n",
    "    Z_exp = np.exp(Z)\n",
    "    Z_sum = np.sum(Z_exp, axis=0, keepdims=True)\n",
    "\n",
    "    H = Z_exp / Z_sum  #normalising step\n",
    "    softmax_memory = Z\n",
    "\n",
    "    return H, softmax_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulative input (Z) : [[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]\n",
      " [12 13 14]\n",
      " [15 16 17]\n",
      " [18 19 20]\n",
      " [21 22 23]\n",
      " [24 25 26]\n",
      " [27 28 29]]\n",
      "\n",
      "output : [[1.78595259e-12 1.78595259e-12 1.78595259e-12]\n",
      " [3.58718166e-11 3.58718166e-11 3.58718166e-11]\n",
      " [7.20504697e-10 7.20504697e-10 7.20504697e-10]\n",
      " [1.44717237e-08 1.44717237e-08 1.44717237e-08]\n",
      " [2.90672341e-07 2.90672341e-07 2.90672341e-07]\n",
      " [5.83831003e-06 5.83831003e-06 5.83831003e-06]\n",
      " [1.17265592e-04 1.17265592e-04 1.17265592e-04]\n",
      " [2.35534237e-03 2.35534237e-03 2.35534237e-03]\n",
      " [4.73083162e-02 4.73083162e-02 4.73083162e-02]\n",
      " [9.50212932e-01 9.50212932e-01 9.50212932e-01]]\n"
     ]
    }
   ],
   "source": [
    "# output layer is made of 10 neurons, testing with 3 input data points\n",
    "Z = np.array(np.arange(30)).reshape(10,3)\n",
    "\n",
    "H, softmax_memory = softmax(Z)\n",
    "\n",
    "# cumulative input to the layer\n",
    "print(\"cumulative input (Z) : {}\\n\".format(softmax_memory))\n",
    "\n",
    "# output of softmax activation function\n",
    "print(\"output : {}\".format(H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. FeedForward\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize_parameters\n",
    "Let's now create a function **`initialize_parameters`** which initializes the weights and biases of the various layers.\n",
    "\n",
    "The inputs to this function is a list named `dimensions`. The length of the list is the number layers in the network + 1 (the plus one is for the input layer, rest are hidden + output). The first element of this list is the dimensionality or length of the input (784 for the MNIST dataset). The rest of the list contains the number of neurons in the corresponding (hidden and output) layers.\n",
    "\n",
    "For example `dimensions = [784, 3, 7, 10]` specifies a network for the MNIST dataset with two hidden layers and a 10-dimensional softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(dimensions):\n",
    "\n",
    "    # dimensions is a list containing the number of neuron in each layer in the network\n",
    "    # It returns parameters which is a python dictionary containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "\n",
    "    np.random.seed(2)\n",
    "    parameters = {}\n",
    "    L = len(dimensions)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(dimensions[l],\n",
    "                                                   dimensions[l - 1]) * 0.1\n",
    "        parameters['b' + str(l)] = np.zeros((dimensions[l], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layer_forward\n",
    "\n",
    "The function **`layer_forward`** implements the forward propagation for a certain layer 'l'. \n",
    "\n",
    "It calculates the cumulative input into the layer **Z** and uses it to calculate the output of the layer __H__. It takes `H_prev`, `W`, `b` and `activation function` as inputs and stores the `linear_memory`, `activation_memory` in the variable `memory` which will be used later in backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_forward(H_prev, W, b, activation='relu'):\n",
    "\n",
    "    # H_prev is of shape (number of neurons in previous layer, number of examples)\n",
    "    # W is weights matrix of shape (number of neurons in current layer, number of neurons in previous layer)\n",
    "    # b is bias vector of shape (number of neurons in current layer, 1) (always a column vector)\n",
    "    # activation is the activation to be used for forward propagation : \"softmax\", \"relu\", \"sigmoid\"\n",
    "\n",
    "    # H is the output of the activation function\n",
    "    # memory is a dictionary containing \"linear_memory\" and \"activation_memory\"\n",
    "\n",
    "    linear_memory = (H_prev, W, b)\n",
    "\n",
    "    Z = np.dot(W, H_prev) + b\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        H, activation_memory = sigmoid(Z)\n",
    "\n",
    "    elif activation == \"softmax\":\n",
    "        H, activation_memory = softmax(Z)\n",
    "\n",
    "    elif activation == \"relu\":\n",
    "        H, activation_memory = relu(Z)\n",
    "\n",
    "    memory = (linear_memory, activation_memory)\n",
    "\n",
    "    return H, memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L_layer_forward\n",
    "\n",
    "**`L_layer_forward`** performs one forward pass through the whole network for all the training samples (note that we are feeding all training examples in one single batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_forward(X, parameters):\n",
    "\n",
    "    # X is input data of shape (input size, number of examples)\n",
    "    # parameters is output of initialize_parameters()\n",
    "    \n",
    "    # HL is the last layer's post-activation value\n",
    "    # memories is the list of memory containing (for a relu activation, for example):\n",
    "    # - every memory of relu forward (there are L-1 of them, indexed from 1 to L-1), \n",
    "    # - the memory of softmax forward (there is one, indexed L) \n",
    "\n",
    "    memories = []\n",
    "    H = X\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    # Implement relu layer (L-1) times as the Lth layer is the softmax layer\n",
    "    for l in range(1, L):\n",
    "        H_prev = H\n",
    "        \n",
    "        H, memory = layer_forward(H_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
    "        \n",
    "        memories.append(memory)\n",
    "    \n",
    "    # Implement the final softmax layer\n",
    "    # HL here is the final prediction P as specified in the lectures\n",
    "    HL, memory = layer_forward(H, parameters['W' + str(L)], parameters['b' + str(L)], \"softmax\")\n",
    "    \n",
    "    memories.append(memory)\n",
    "\n",
    "    # number of data points in the output should be same as those present in the input\n",
    "    assert(HL.shape == (10, X.shape[1]))\n",
    "            \n",
    "    return HL, memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Network Loss\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute_loss\n",
    "**`compute_loss`** calculates the cross-entropy loss of a single pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(HL, Y):\n",
    "\n",
    "    # HL is probability matrix of shape (10, number of examples)\n",
    "    # Y is true \"label\" vector shape (10, number of examples)\n",
    "\n",
    "    # loss is the cross-entropy loss\n",
    "\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    loss = -np.divide(np.sum(np.multiply(Y, np.log(HL))), m)\n",
    "\n",
    "    # To make sure that the loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    loss = np.squeeze(loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Backpropagation\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid-backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dH, sigmoid_memory):\n",
    "    \n",
    "    # Implement the backpropagation of a sigmoid function\n",
    "    # dH is gradient of the sigmoid activated activation of shape same as H or Z in the same layer    \n",
    "    # sigmoid_memory is the memory stored in the sigmoid(Z) calculation\n",
    "    \n",
    "    Z = sigmoid_memory\n",
    "    \n",
    "    H = 1/(1+np.exp(-Z))\n",
    "    \n",
    "    # dZ is the gradient of the loss function with respect to cumulative input Z to a layer 'l'\n",
    "    # dZ is calculated as Hadamard product of cumulative input 'Z' and differential of activation function\n",
    "    # in the below formula, (H * (1 - H) ) represents the differential of 'sigmoid' activation function\n",
    "    dZ = dH * H * (1-H)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relu-backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dH, relu_memory):\n",
    "    \n",
    "    # Implement the backpropagation of a relu function\n",
    "    # dH is gradient of the relu activated activation of shape same as H or Z in the same layer    \n",
    "    # relu_memory is the memory stored in the sigmoid(Z) calculation\n",
    "    \n",
    "    Z = relu_memory\n",
    "    \n",
    "    dZ = np.array(dH, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layer_backward\n",
    "\n",
    "**`layer_backward`** is a complimentary function of **`layer_forward`**. Like `layer_forward`, `layer_backward` uses **dH** to calculate **dW**, **dH_prev** and **db**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_backward(dH, memory, activation='relu'):\n",
    "\n",
    "    # takes dH and the memory calculated in layer_forward and activation as input to calculate the dH_prev, dW, db\n",
    "    # performs the backpropagation depending upon the activation function\n",
    "\n",
    "    linear_memory, activation_memory = memory\n",
    "    H_prev, W, b = linear_memory\n",
    "    \n",
    "    # number of data points present in the input batch\n",
    "    m = H_prev.shape[1]\n",
    "    \n",
    "    # generated gradient of weight, dW, is a tensor. That of bias, db, is a matrix\n",
    "    #\n",
    "    # hence both must be divided by the number of input data points to squish \n",
    "    # those into a matrix and vector respectively\n",
    "    # \n",
    "    # this will ensure that shape of a gradient is same as corresponding weight and bias matrix\n",
    "    # for parameter updates\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dH, activation_memory)\n",
    "        \n",
    "        dW = np.divide(np.dot(dZ, H_prev.T), m)\n",
    "        db = np.divide(np.sum(dZ, axis=1, keepdims=True), m)\n",
    "        \n",
    "        dH_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dH, activation_memory)\n",
    "\n",
    "        dW = np.divide(np.dot(dZ, H_prev.T), m)\n",
    "        db = np.divide(np.sum(dZ, axis=1, keepdims=True), m)\n",
    "        \n",
    "        dH_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dH_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L_layer_backward\n",
    "\n",
    "**`L_layer_backward`** performs backpropagation for the whole network. Recall that the backpropagation for the last layer, i.e. the softmax layer, is different from the rest, hence it is outside the reversed `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_backward(HL, Y, memories):\n",
    "    \n",
    "    # Following are the inputs:\n",
    "    #       predicted value HL\n",
    "    #       true target value Y\n",
    "    #       memories calculated by L_layer_forward\n",
    "    \n",
    "    # returns the gradients calulated for all the layers as a dict\n",
    "\n",
    "    gradients = {}\n",
    "    \n",
    "    # number of layers in the network\n",
    "    L = len(memories)\n",
    "    \n",
    "    # number of data points\n",
    "    m = HL.shape[1]\n",
    "    \n",
    "    # ensure shape of ground truth is same as calculated output\n",
    "    Y = Y.reshape(HL.shape)\n",
    "    \n",
    "    # Perform backprop of output softmax layer\n",
    "    current_memory = memories[-1]\n",
    "    linear_memory, activation_memory = current_memory\n",
    "    dZ = HL - Y\n",
    "    H_prev, W, b = linear_memory\n",
    "    \n",
    "    gradients[\"dH\" + str(L-1)] = np.dot(W.T, dZ)\n",
    "    gradients[\"dW\" + str(L)] = np.divide(np.dot(dZ, H_prev.T), m)\n",
    "    gradients[\"db\" + str(L)] = np.divide(np.sum(dZ, axis=1, keepdims=True), m)\n",
    "    \n",
    "    # Perform the backpropagation l-1 times\n",
    "    for l in reversed(range(L-1)):\n",
    "        \n",
    "        current_memory = memories[l]\n",
    "        \n",
    "        dH_prev_temp, dW_temp, db_temp = layer_backward(gradients[\"dH\" + str(l+1)], current_memory, 'relu')\n",
    "        gradients[\"dH\" + str(l)] = dH_prev_temp\n",
    "        gradients[\"dW\" + str(l + 1)] = dW_temp\n",
    "        gradients[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Parameter Updates\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "\n",
    "    # parameters is a dictionary containing the parameters W and b for all the layers\n",
    "    # gradients is a dictionary containing the gradient outputs of L_model_backward\n",
    "\n",
    "    # returns updated weights after applying the gradient descent update\n",
    "\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = np.subtract(\n",
    "            parameters[\"W\" + str(l + 1)],\n",
    "            np.multiply(learning_rate, gradients[\"dW\" + str(l + 1)]))\n",
    "        \n",
    "        parameters[\"b\" + str(l + 1)] = np.subtract(\n",
    "            parameters[\"b\" + str(l + 1)],\n",
    "            np.multiply(learning_rate, gradients[\"db\" + str(l + 1)]))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the bits and pieces of the feedforward and the backpropagation, let's now combine all that to form a model. The list `dimensions` has the number of neurons in each layer specified in it. For a neural network with 1 hidden layer with 45 neurons, you would specify the dimensions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [784, 45, 10] #  three-layer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Model\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L_layer_model\n",
    "a composite function which takes the following input paramters:\n",
    "\n",
    "  - training data as input **X**\n",
    "  - ground truth label **Y**\n",
    "  - **dimensions** as stated above\n",
    "  - **learning_rate**\n",
    "  - the number of iterations **num_iterations**\n",
    "  - if you want to print the loss, **print_loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, dimensions, learning_rate = 0.0075, num_iterations = 3000, print_loss=False):\n",
    "    \n",
    "    np.random.seed(2)\n",
    "    losses = []\n",
    "    \n",
    "    # Parameters initialization\n",
    "    parameters = initialize_parameters(dimensions)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation\n",
    "        HL, memories = L_layer_forward(X, parameters)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = compute_loss(HL, Y)\n",
    "    \n",
    "        # Backward propagation\n",
    "        gradients = L_layer_backward(HL, Y, memories)\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "                \n",
    "        # Printing the loss every 200 iterations\n",
    "        if print_loss and i % 200 == 0:\n",
    "            print (\"Loss after iteration %i: %f\" %(i, loss))\n",
    "            losses.append(loss)\n",
    "            \n",
    "    # plotting the loss\n",
    "    plt.plot(np.squeeze(losses))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, it'll take a lot of time to train the model on 50,000 data points, we take a subset of 10,000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 10000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_x_new = train_set_x[:,0:10000]\n",
    "train_set_y_new = train_set_y[:,0:10000]\n",
    "train_set_x_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Training\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 2.420266\n",
      "Loss after iteration 200: 1.874997\n",
      "Loss after iteration 400: 1.355493\n",
      "Loss after iteration 600: 1.000101\n",
      "Loss after iteration 800: 0.802721\n",
      "Loss after iteration 1000: 0.686624\n",
      "Loss after iteration 1200: 0.611321\n",
      "Loss after iteration 1400: 0.558733\n",
      "Loss after iteration 1600: 0.519849\n",
      "Loss after iteration 1800: 0.489858\n",
      "Loss after iteration 2000: 0.465936\n",
      "Loss after iteration 2200: 0.446298\n",
      "Loss after iteration 2400: 0.429824\n",
      "Loss after iteration 2600: 0.415757\n",
      "Loss after iteration 2800: 0.403538\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to complete training : 774.2483738999999\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "parameters = L_layer_model(train_set_x_new, train_set_y_new, dimensions, num_iterations = 3000, print_loss = True)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print(\"Time taken to complete training : {}\".format(stop - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
