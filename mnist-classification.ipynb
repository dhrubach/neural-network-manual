{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "import h5py\n",
    "import scipy\n",
    "\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load MNIST Data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    f.seek(0)\n",
    "    \n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "    return (training_data, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the data looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([5, 0, 4, ..., 8, 4, 8], dtype=int64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "# shape of data\n",
    "print(training_data[0].shape)\n",
    "print(training_data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature dataset : [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Target dataset : [5 0 4 ... 8 4 8]\n",
      "\n",
      "Number of examples in the training dataset : 50000\n",
      "Number of points in a single input : 784\n",
      "\n",
      "Number of elements in the target dataset : 50000\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature dataset : {}\".format(training_data[0]))\n",
    "print(\"Target dataset : {}\\n\".format(training_data[1]))\n",
    "\n",
    "print(\"Number of examples in the training dataset : {}\".format(len(training_data[0])))\n",
    "print(\"Number of points in a single input : {}\\n\".format(len(training_data[0][1])))\n",
    "\n",
    "print(\"Number of elements in the target dataset : {}\".format(len(training_data[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> -  *Number of images present in the training dataset : 50,000*\n",
    "-  *Each image is represented by 784 data points representing a 28 X 28 image*\n",
    "-  *Each element in target dataset represents expected image label*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preparation\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert output image label into one-hot encoded column vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(j):\n",
    "    # get the number of elements in the input array\n",
    "    n = j.shape[0]\n",
    "\n",
    "    # creates a 10 X n array \n",
    "    # each row represents a digit i.e. 10 rows (0 - 9)\n",
    "    # each column represents an input\n",
    "    new_array = np.zeros((10, n))\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    for res in j:\n",
    "        new_array[res][index] = 1.0\n",
    "        index = index + 1\n",
    "    \n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert datasets into desired shapes and the ground truth labels to one_hot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_wrapper(tr_d, va_d, te_d):\n",
    "\n",
    "    # transpose input matrix so that each input is represented by a column vector\n",
    "    training_inputs = np.array(tr_d[0][:]).T\n",
    "    validation_inputs = np.array(va_d[0][:]).T\n",
    "    test_inputs = np.array(te_d[0][:]).T\n",
    "\n",
    "    # generate one-hot encoded matrix for output labels\n",
    "    training_results = np.array(tr_d[1][:])\n",
    "    train_set_y = one_hot(training_results)\n",
    "\n",
    "    validation_results = np.array(va_d[1][:])\n",
    "    validation_set_y = one_hot(validation_results)\n",
    "\n",
    "    test_results = np.array(te_d[1][:])\n",
    "    test_set_y = one_hot(test_results)\n",
    "\n",
    "    return (training_inputs, train_set_y, test_inputs, test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x, train_set_y, test_set_x, test_set_y = data_wrapper(\n",
    "    training_data, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x shape: (784, 50000)\n",
      "train_set_y shape: (10, 50000)\n",
      "test_set_x shape: (784, 10000)\n",
      "test_set_y shape: (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "print (\"train_set_x shape: \" + str(train_set_x.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data_wrapper has converted the training and validation data into numpy array of desired shapes. Let's convert the actual labels into a dataframe to see if the one hot conversions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target dataset is:[5 0 4 ... 8 4 8]\n",
      "The one hot encoding dataset is:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>49990</th>\n",
       "      <th>49991</th>\n",
       "      <th>49992</th>\n",
       "      <th>49993</th>\n",
       "      <th>49994</th>\n",
       "      <th>49995</th>\n",
       "      <th>49996</th>\n",
       "      <th>49997</th>\n",
       "      <th>49998</th>\n",
       "      <th>49999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 50000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3      4      5      6      7      8      9      \\\n",
       "0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1    0.0    0.0    0.0    1.0    0.0    0.0    1.0    0.0    1.0    0.0   \n",
       "2    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0   \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0   \n",
       "4    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0   \n",
       "5    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "6    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "7    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "8    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "9    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   ...    49990  49991  49992  49993  49994  49995  49996  49997  49998  49999  \n",
       "0  ...      0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0  \n",
       "1  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2  ...      0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4  ...      0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    1.0    0.0  \n",
       "5  ...      0.0    1.0    1.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0  \n",
       "6  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "7  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "8  ...      1.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    1.0  \n",
       "9  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[10 rows x 50000 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The target dataset is:\" + str(training_data[1]))\n",
    "print(\"The one hot encoding dataset is:\")\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualise the dataset. Feel free to change the index to see if the training data has been correctly tagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2311757add8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index  = 49991\n",
    "k = train_set_x[:,index]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label= training_data[1][index]))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Activation Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid\n",
    "It takes a cumulative input into the layer, the matrix __Z__, as the input. Upon application of `sigmoid` function, the output matrix **H** is calculated. Also, Z is stored as the variable sigmoid_memory since it will be later used in backpropagation. You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ here in the following way. The exponential gets applied to all the elements of Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "\n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples\n",
    "    # sigmoid_memory is stored as it is used later on in backpropagation\n",
    "\n",
    "    H = 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    sigmoid_memory = Z\n",
    "\n",
    "    return H, sigmoid_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5        0.88079708 0.98201379 0.99752738]\n",
      "[0.5        0.73105858]\n"
     ]
    }
   ],
   "source": [
    "# 4 neurons in a layer, 2 input data points\n",
    "Z = np.arange(8).reshape(4, 2)\n",
    "H, sigmoid_memory = sigmoid(Z)\n",
    "\n",
    "# output of 1st input data points\n",
    "print(H[:, 0])\n",
    "\n",
    "# output of top neuron\n",
    "print(H[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relu\n",
    "It takes the cumulative input to the layer, matrix **Z** as the input. Upon application of the `relu` function, matrix __H__ which is the output matrix is calculated. Also, Z is stored as `relu_memory` which will be later used in backpropagation. You use _[np.maximum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.maximum.html)_ here in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples\n",
    "    # relu_memory is stored as it is used later on in backpropagation\n",
    "\n",
    "    H = np.maximum(0, Z)\n",
    "\n",
    "    assert (H.shape == Z.shape)\n",
    "\n",
    "    relu_memory = Z\n",
    "    return H, relu_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulative input (Z) : [[ 1  3]\n",
      " [-1 -4]\n",
      " [-5  7]\n",
      " [ 9 18]]\n",
      "\n",
      "output : [[ 1  3]\n",
      " [ 0  0]\n",
      " [ 0  7]\n",
      " [ 9 18]]\n"
     ]
    }
   ],
   "source": [
    "# 4 neurons in a layer, 2 input data points\n",
    "Z = np.array([1, 3, -1, -4, -5, 7, 9, 18]).reshape(4, 2)\n",
    "\n",
    "H, relu_memory = relu(Z)\n",
    "\n",
    "# cumulative input to the layer\n",
    "print(\"cumulative input (Z) : {}\\n\".format(relu_memory))\n",
    "\n",
    "# output of relu activation function\n",
    "print(\"output : {}\".format(H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax\n",
    "It takes the cumulative input to the layer, matrix **Z** as the input. Upon application of the `softmax` function, the output matrix __H__ is calculated. Also, Z is stored as `softmax_memory` which will be later used in backpropagation. You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ and _[np.sum()](https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.sum.html)_ here in the following way. The exponential gets applied to all the elements of Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples\n",
    "    # softmax_memory is stored as it is used later on in backpropagation\n",
    "\n",
    "    Z_exp = np.exp(Z)\n",
    "    Z_sum = np.sum(Z_exp, axis=0, keepdims=True)\n",
    "\n",
    "    H = Z_exp / Z_sum  #normalising step\n",
    "    softmax_memory = Z\n",
    "\n",
    "    return H, softmax_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulative input (Z) : [[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]\n",
      " [12 13 14]\n",
      " [15 16 17]\n",
      " [18 19 20]\n",
      " [21 22 23]\n",
      " [24 25 26]\n",
      " [27 28 29]]\n",
      "\n",
      "output : [[1.78595259e-12 1.78595259e-12 1.78595259e-12]\n",
      " [3.58718166e-11 3.58718166e-11 3.58718166e-11]\n",
      " [7.20504697e-10 7.20504697e-10 7.20504697e-10]\n",
      " [1.44717237e-08 1.44717237e-08 1.44717237e-08]\n",
      " [2.90672341e-07 2.90672341e-07 2.90672341e-07]\n",
      " [5.83831003e-06 5.83831003e-06 5.83831003e-06]\n",
      " [1.17265592e-04 1.17265592e-04 1.17265592e-04]\n",
      " [2.35534237e-03 2.35534237e-03 2.35534237e-03]\n",
      " [4.73083162e-02 4.73083162e-02 4.73083162e-02]\n",
      " [9.50212932e-01 9.50212932e-01 9.50212932e-01]]\n"
     ]
    }
   ],
   "source": [
    "# output layer is made of 10 neurons, testing with 3 input data points\n",
    "Z = np.array(np.arange(30)).reshape(10,3)\n",
    "\n",
    "H, softmax_memory = softmax(Z)\n",
    "\n",
    "# cumulative input to the layer\n",
    "print(\"cumulative input (Z) : {}\\n\".format(softmax_memory))\n",
    "\n",
    "# output of softmax activation function\n",
    "print(\"output : {}\".format(H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. FeedForward\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize_parameters\n",
    "Let's now create a function **`initialize_parameters`** which initializes the weights and biases of the various layers.\n",
    "\n",
    "The inputs to this function is a list named `dimensions`. The length of the list is the number layers in the network + 1 (the plus one is for the input layer, rest are hidden + output). The first element of this list is the dimensionality or length of the input (784 for the MNIST dataset). The rest of the list contains the number of neurons in the corresponding (hidden and output) layers.\n",
    "\n",
    "For example `dimensions = [784, 3, 7, 10]` specifies a network for the MNIST dataset with two hidden layers and a 10-dimensional softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(dimensions):\n",
    "\n",
    "    # dimensions is a list containing the number of neuron in each layer in the network\n",
    "    # It returns parameters which is a python dictionary containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "\n",
    "    np.random.seed(2)\n",
    "    parameters = {}\n",
    "    L = len(dimensions)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(dimensions[l],\n",
    "                                                   dimensions[l - 1]) * 0.1\n",
    "        parameters['b' + str(l)] = np.zeros((dimensions[l], 1))\n",
    "\n",
    "        assert (parameters['W' + str(l)].shape == (dimensions[l],\n",
    "                                                   dimensions[l - 1]))\n",
    "        assert (parameters['b' + str(l)].shape == (dimensions[l], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layer_forward\n",
    "\n",
    "The function **`layer_forward`** implements the forward propagation for a certain layer 'l'. \n",
    "\n",
    "It calculates the cumulative input into the layer **Z** and uses it to calculate the output of the layer __H__. It takes H_prev, W, b and the activation function as inputs and stores the linear_memory, activation_memory in the variable memory which will be used later in backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_forward(H_prev, W, b, activation='relu'):\n",
    "\n",
    "    # H_prev is of shape (size of previous layer, number of examples)\n",
    "    # W is weights matrix of shape (size of current layer, size of previous layer)\n",
    "    # b is bias vector of shape (size of the current layer, 1)\n",
    "    # activation is the activation to be used for forward propagation : \"softmax\", \"relu\", \"sigmoid\"\n",
    "\n",
    "    # H is the output of the activation function\n",
    "    # memory is a python dictionary containing \"linear_memory\" and \"activation_memory\"\n",
    "\n",
    "    linear_memory = (H_prev, W, b)\n",
    "\n",
    "    Z = np.dot(W, H_prev) + b\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        H, activation_memory = sigmoid(Z)\n",
    "\n",
    "    elif activation == \"softmax\":\n",
    "        H, activation_memory = softmax(Z)\n",
    "\n",
    "    elif activation == \"relu\":\n",
    "        H, activation_memory = relu(Z)\n",
    "\n",
    "    memory = (linear_memory, activation_memory)\n",
    "\n",
    "    return H, memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L_layer_forward\n",
    "\n",
    "**`L_layer_forward`** performs one forward pass through the whole network for all the training samples (note that we are feeding all training examples in one single batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_forward(X, parameters):\n",
    "\n",
    "    # X is input data of shape (input size, number of examples)\n",
    "    # parameters is output of initialize_parameters()\n",
    "    \n",
    "    # HL is the last layer's post-activation value\n",
    "    # memories is the list of memory containing (for a relu activation, for example):\n",
    "    # - every memory of relu forward (there are L-1 of them, indexed from 1 to L-1), \n",
    "    # - the memory of softmax forward (there is one, indexed L) \n",
    "\n",
    "    memories = []\n",
    "    H = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement relu layer (L-1) times as the Lth layer is the softmax layer\n",
    "    for l in range(1, L):\n",
    "        H_prev = H\n",
    "        \n",
    "        H, memory = layer_forward(H_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
    "        \n",
    "        memories.append(memory)\n",
    "    \n",
    "    # Implement the final softmax layer\n",
    "    # HL here is the final prediction P as specified in the lectures\n",
    "    HL, memory = layer_forward(H, parameters['W' + str(L)], parameters['b' + str(L)], \"softmax\")\n",
    "    \n",
    "    memories.append(memory)\n",
    "\n",
    "    assert(HL.shape == (10, X.shape[1]))\n",
    "            \n",
    "    return HL, memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Network Loss\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
